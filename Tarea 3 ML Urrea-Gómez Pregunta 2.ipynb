{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> INF-393 Máquinas de Aprendizaje II-2018 </h1>\n",
    "\n",
    "<H3 align='center'> Fernanda Urrea, ROL: 201551522-0 </H3>\n",
    "<H3 align='center'> Matías Gómez, ROL: 201460501-3 </H3>\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Redes Convolucionales sobre imágenes\n",
    "---\n",
    "Las redes neuronales hoy en día han sido extendidas a numerosas aplicaciones gracias a la arquitectura definida para cada tipo de problema. Las redes neuronales que aplican la operación de convolución [[3]](#refs) o convoluciones en sus capas son concidas como *CNN* o *ConvNets*, lo cual se especializa en trabajar en datos con forma matricial (ya sea bi-dimensional o tri-dimensional), lo cual se adecúa perfectamente a imágenes (matrices), ya que gracias a su conectividad local se especializan en reconocer patrones sobre los datos de manera espacial, como refleja la siguiente imagen:\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*N4h1SgwbWNmtrRhszM9EJg.png\" title=\"Title text\" width=\"90%\" />\n",
    "\n",
    "\n",
    "En esta actividad trabajará con un extracto bastante pequeño del dataset conocido como **101-Food**[[4]](#refs), el cual consta de mil imágenes pertenecientes a 3 clases (*Hambuger, Hot Dog* y *Pizza*) separados en conjunto de entrenamiento y validación.  \n",
    "El extracto pequeño del dataset con el que se trabajará deberá ser descargado del siguiente __[link](https://www.dropbox.com/s/56xqazmhbh0doi7/food_data.zip?dl=0)__ a través de Dropbox."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Construya funciones para leer los datos y cargarlos al momento de entrenar (durante cada *epoch*), para ésto utilice *Image Data Generator* de keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2400 images belonging to 3 classes.\n",
      "Found 600 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255) #no transformation\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'food_data/train',\n",
    "        target_size=(150, 150),\n",
    "        color_mode='rgb',\n",
    "        batch_size=32)\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        'food_data/val',\n",
    "        target_size=(150, 150),\n",
    "        color_mode='rgb',\n",
    "        batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la función ImageDataGenerator genera grupos de arreglos de datos de imagenes con aumentos de datos de imágenes en tiempo real.  \n",
    "El parámetro rescale es un factor de reescalamiento que multiplica los datos.  \n",
    "El parámetro shear_range corta imágenes.  \n",
    "El parámetro zoom_range amplía imágenes.  \n",
    "El parámetro horizontal_flip  orienta las imágenes de forma horizontal de manera aleatoria.  \n",
    "  \n",
    "Luego se aplican estas funciones para generar un conjunto de entrenamiento y un conjunto de pruebas llamados train_generator y validation_generator respectivamente, que serviran para cargar los datos al momento de entrenar.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Utilice la red tradicional (*Feed Forward*) entregada en el código para ser entrenada sobre los datos vectorizados, esto es que cada imagen queda representada como un vector gigante, y las 3 clases a las que se enfrenta. Evalúe el modelo con la métrica *accuracy* sobre el conjunto de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 67500)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 67500)             270000    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               17280256  \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 17,583,539\n",
      "Trainable params: 17,448,539\n",
      "Non-trainable params: 135,000\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "75/75 [==============================] - 79s 1s/step - loss: 8.6530 - acc: 0.4371 - val_loss: 8.7369 - val_acc: 0.4525\n",
      "Epoch 2/25\n",
      "75/75 [==============================] - 78s 1s/step - loss: 8.3711 - acc: 0.4742 - val_loss: 9.1693 - val_acc: 0.4296\n",
      "Epoch 3/25\n",
      "75/75 [==============================] - 75s 1s/step - loss: 8.4590 - acc: 0.4688 - val_loss: 8.8408 - val_acc: 0.4507\n",
      "Epoch 4/25\n",
      "75/75 [==============================] - 74s 991ms/step - loss: 8.5792 - acc: 0.4637 - val_loss: 9.2471 - val_acc: 0.4243\n",
      "Epoch 5/25\n",
      "75/75 [==============================] - 84s 1s/step - loss: 9.0067 - acc: 0.4375 - val_loss: 9.5290 - val_acc: 0.4049\n",
      "Epoch 6/25\n",
      "75/75 [==============================] - 74s 986ms/step - loss: 9.0033 - acc: 0.4379 - val_loss: 9.3776 - val_acc: 0.4137\n",
      "Epoch 7/25\n",
      "75/75 [==============================] - 75s 1s/step - loss: 9.1768 - acc: 0.4283 - val_loss: 9.8111 - val_acc: 0.3906\n",
      "Epoch 8/25\n",
      "75/75 [==============================] - 74s 991ms/step - loss: 9.3548 - acc: 0.4158 - val_loss: 9.9324 - val_acc: 0.3838\n",
      "Epoch 9/25\n",
      "75/75 [==============================] - 73s 970ms/step - loss: 9.7292 - acc: 0.3925 - val_loss: 9.9823 - val_acc: 0.3732\n",
      "Epoch 10/25\n",
      "75/75 [==============================] - 80s 1s/step - loss: 9.1971 - acc: 0.4275 - val_loss: 9.5821 - val_acc: 0.4032\n",
      "Epoch 11/25\n",
      "75/75 [==============================] - 79s 1s/step - loss: 9.3670 - acc: 0.4171 - val_loss: 9.8193 - val_acc: 0.3908\n",
      "Epoch 12/25\n",
      "75/75 [==============================] - 73s 979ms/step - loss: 9.2577 - acc: 0.4237 - val_loss: 8.7726 - val_acc: 0.4525\n",
      "Epoch 13/25\n",
      "75/75 [==============================] - 71s 949ms/step - loss: 8.7151 - acc: 0.4571 - val_loss: 8.6787 - val_acc: 0.4577\n",
      "Epoch 14/25\n",
      "75/75 [==============================] - 74s 989ms/step - loss: 8.5645 - acc: 0.4671 - val_loss: 8.5984 - val_acc: 0.4665\n",
      "Epoch 15/25\n",
      "75/75 [==============================] - 77s 1s/step - loss: 8.7258 - acc: 0.4558 - val_loss: 9.0522 - val_acc: 0.4384\n",
      "Epoch 16/25\n",
      "75/75 [==============================] - 73s 970ms/step - loss: 9.0367 - acc: 0.4363 - val_loss: 9.3383 - val_acc: 0.4190\n",
      "Epoch 17/25\n",
      "75/75 [==============================] - 75s 1s/step - loss: 8.8902 - acc: 0.4471 - val_loss: 8.4452 - val_acc: 0.4736\n",
      "Epoch 18/25\n",
      "75/75 [==============================] - 76s 1s/step - loss: 8.9777 - acc: 0.4429 - val_loss: 8.7865 - val_acc: 0.4542\n",
      "Epoch 19/25\n",
      "75/75 [==============================] - 74s 984ms/step - loss: 8.9174 - acc: 0.4462 - val_loss: 9.1404 - val_acc: 0.4313\n",
      "Epoch 20/25\n",
      "75/75 [==============================] - 73s 972ms/step - loss: 8.9401 - acc: 0.4442 - val_loss: 8.9626 - val_acc: 0.4437\n",
      "Epoch 21/25\n",
      "75/75 [==============================] - 73s 969ms/step - loss: 9.1012 - acc: 0.4346 - val_loss: 9.2020 - val_acc: 0.4278\n",
      "Epoch 22/25\n",
      "75/75 [==============================] - 73s 969ms/step - loss: 8.9508 - acc: 0.4442 - val_loss: 9.6482 - val_acc: 0.4014\n",
      "Epoch 23/25\n",
      "75/75 [==============================] - 74s 981ms/step - loss: 9.0931 - acc: 0.4350 - val_loss: 8.9104 - val_acc: 0.4472\n",
      "Epoch 24/25\n",
      "75/75 [==============================] - 73s 979ms/step - loss: 9.0836 - acc: 0.4363 - val_loss: 9.1090 - val_acc: 0.4349\n",
      "Epoch 25/25\n",
      "75/75 [==============================] - 77s 1s/step - loss: 9.2210 - acc: 0.4279 - val_loss: 9.2627 - val_acc: 0.4243\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20780236400>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=train_generator.image_shape)) #full dense\n",
    "model.add(BatchNormalization()) #to normalize the input..\n",
    "model.add(Dense(256,activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(128,activation='relu')) #128\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(3,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy']) # Let's train the model using RMSprop\n",
    "model.summary()\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator.classes)//train_generator.batch_size, #samples//batch_size\n",
    "        epochs=25,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator.classes)//validation_generator.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy validation:  0.484375\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy validation: \",model.evaluate_generator(generator=validation_generator, steps=2)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy validation:  0.41919191919191917\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy validation: \",model.evaluate_generator(generator=validation_generator, steps=25)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy validation:  0.425126903099457\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy validation: \",model.evaluate_generator(generator=validation_generator, steps=50)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy validation:  0.4375\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy validation: \",model.evaluate_generator(generator=validation_generator, steps=10)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tal como se pide, se utiliza una red neuronal Feed Forward tradicional para el aprendizaje y se mide el desempeño en el conjunto de validación, entregando un accuracy de entre 0,419 y 0,485.  \n",
    "Cabe destacar que en una red Feed forward tradicional, todas las neuronas están conectadas.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Utilice la red convolucional (**CNN**) entregada en el código para ser entrenada sobre los datos brutos, matrices RGB de píxeles, y las 3 clases a las que se enfrenta. Evalúe el modelo con la métrica *accuracy* sobre el conjunto de validación. Compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 150, 150, 32)      896       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 148, 148, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 74, 74, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 72, 72, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 82944)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               10616960  \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 10,682,915\n",
      "Trainable params: 10,682,915\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "75/75 [==============================] - 516s 7s/step - loss: 1.2790 - acc: 0.3829 - val_loss: 1.0533 - val_acc: 0.3926\n",
      "Epoch 2/25\n",
      "75/75 [==============================] - 639s 9s/step - loss: 1.0537 - acc: 0.4617 - val_loss: 1.0158 - val_acc: 0.4736\n",
      "Epoch 3/25\n",
      "75/75 [==============================] - 486s 6s/step - loss: 1.0151 - acc: 0.5204 - val_loss: 0.9671 - val_acc: 0.5704\n",
      "Epoch 4/25\n",
      "75/75 [==============================] - 492s 7s/step - loss: 0.9947 - acc: 0.5288 - val_loss: 0.9669 - val_acc: 0.5194\n",
      "Epoch 5/25\n",
      "75/75 [==============================] - 492s 7s/step - loss: 0.9652 - acc: 0.5608 - val_loss: 0.9594 - val_acc: 0.5827\n",
      "Epoch 6/25\n",
      "75/75 [==============================] - 481s 6s/step - loss: 0.9085 - acc: 0.5800 - val_loss: 0.9479 - val_acc: 0.5528\n",
      "Epoch 7/25\n",
      "75/75 [==============================] - 479s 6s/step - loss: 0.8811 - acc: 0.5979 - val_loss: 1.0227 - val_acc: 0.4894\n",
      "Epoch 8/25\n",
      "75/75 [==============================] - 479s 6s/step - loss: 0.8685 - acc: 0.6050 - val_loss: 0.9773 - val_acc: 0.5352\n",
      "Epoch 9/25\n",
      "75/75 [==============================] - 475s 6s/step - loss: 0.8303 - acc: 0.6446 - val_loss: 1.2072 - val_acc: 0.4894\n",
      "Epoch 10/25\n",
      "75/75 [==============================] - 474s 6s/step - loss: 0.8078 - acc: 0.6437 - val_loss: 0.8000 - val_acc: 0.6496\n",
      "Epoch 11/25\n",
      "75/75 [==============================] - 480s 6s/step - loss: 0.7927 - acc: 0.6650 - val_loss: 0.8347 - val_acc: 0.6479\n",
      "Epoch 12/25\n",
      "75/75 [==============================] - 482s 6s/step - loss: 0.7628 - acc: 0.6796 - val_loss: 0.7744 - val_acc: 0.6673\n",
      "Epoch 13/25\n",
      "75/75 [==============================] - 479s 6s/step - loss: 0.7325 - acc: 0.6787 - val_loss: 0.7592 - val_acc: 0.6373\n",
      "Epoch 14/25\n",
      "75/75 [==============================] - 486s 6s/step - loss: 0.7182 - acc: 0.7013 - val_loss: 0.7589 - val_acc: 0.6866\n",
      "Epoch 15/25\n",
      "75/75 [==============================] - 477s 6s/step - loss: 0.6971 - acc: 0.6987 - val_loss: 0.7162 - val_acc: 0.6884\n",
      "Epoch 16/25\n",
      "75/75 [==============================] - 480s 6s/step - loss: 0.6806 - acc: 0.7113 - val_loss: 1.0189 - val_acc: 0.5792\n",
      "Epoch 17/25\n",
      "75/75 [==============================] - 480s 6s/step - loss: 0.6591 - acc: 0.7225 - val_loss: 0.8111 - val_acc: 0.6458\n",
      "Epoch 18/25\n",
      "75/75 [==============================] - 479s 6s/step - loss: 0.6988 - acc: 0.7154 - val_loss: 0.7321 - val_acc: 0.6866\n",
      "Epoch 19/25\n",
      "75/75 [==============================] - 477s 6s/step - loss: 0.6489 - acc: 0.7304 - val_loss: 0.6436 - val_acc: 0.7289\n",
      "Epoch 20/25\n",
      "75/75 [==============================] - 478s 6s/step - loss: 0.6552 - acc: 0.7288 - val_loss: 0.7382 - val_acc: 0.6954\n",
      "Epoch 21/25\n",
      "75/75 [==============================] - 478s 6s/step - loss: 0.6461 - acc: 0.7479 - val_loss: 0.8429 - val_acc: 0.6549\n",
      "Epoch 22/25\n",
      "75/75 [==============================] - 483s 6s/step - loss: 0.6359 - acc: 0.7475 - val_loss: 0.6581 - val_acc: 0.7183\n",
      "Epoch 23/25\n",
      "75/75 [==============================] - 480s 6s/step - loss: 0.6215 - acc: 0.7567 - val_loss: 0.7190 - val_acc: 0.6866\n",
      "Epoch 24/25\n",
      "75/75 [==============================] - 477s 6s/step - loss: 0.6221 - acc: 0.7671 - val_loss: 1.0822 - val_acc: 0.6162\n",
      "Epoch 25/25\n",
      "75/75 [==============================] - 482s 6s/step - loss: 0.5884 - acc: 0.7696 - val_loss: 0.6019 - val_acc: 0.7359\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20780a58240>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',input_shape=train_generator.image_shape,activation='relu'))\n",
    "model.add(Conv2D(32, (3, 3),activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, (3, 3), padding='same',activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3),activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dropout(0.5)) \n",
    "model.add(Dense(len(train_generator.class_indices),activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy']) # Let's train the model using RMSprop\n",
    "model.summary()\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator.classes)//train_generator.batch_size, #samples//batch_size\n",
    "        epochs=25,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator.classes)//validation_generator.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy validation:  0.734375\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy validation: \",model.evaluate_generator(generator=validation_generator, steps=2)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy validation:  0.7395833333333334\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy validation: \",model.evaluate_generator(generator=validation_generator, steps=3)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy validation:  0.7578125\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy validation: \",model.evaluate_generator(generator=validation_generator, steps=4)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como era de esperarse ya que el problema es de clasificación de imágenes, el accuracy en el conjunto de validación aumentó al usar CNN.  La accuracy de la CNN en el conjunto de validación es mayor a 0,734.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Genere un conjunto datos con incorrecta etiquetación de manera manual y vea si el modelo convolucional se sigue comportando de la misma manera. Para esto tome 100 imágenes aleatorias de entrenamiento de la carpeta *hot dog* y 100 imágenes aleatorias de entrenamiento de la carpeta *hamburger* e intercambielas, sin manipular las imágenes de la carpeta *pizza* y con el conjunto de validación intacto. Genere las matrices de confusión en el conjunto de validación para visualizar cómo afectó al modelo la corrupción realizada a los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix \n",
    "Y_pred_1 = model.predict_generator(validation_generator,len(validation_generator.classes)//validation_generator.batch_size+1)\n",
    "y_pred_1 = np.argmax(Y_pred_1, axis=1)\n",
    "confusion_matrix = confusion_matrix(validation_generator.classes, y_pred_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[66, 58, 76],\n",
       "       [61, 61, 78],\n",
       "       [70, 69, 61]], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se imprime la matriz de validación de la CNN con los datos bien clasificados.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se realizaron los cambios en las carpetas para generar etiquetación incorrecta.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2400 images belonging to 3 classes.\n",
      "Found 600 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255) #no transformation\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'food_data/train',\n",
    "        target_size=(150, 150),\n",
    "        color_mode='rgb',\n",
    "        batch_size=32)\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        'food_data/val',\n",
    "        target_size=(150, 150),\n",
    "        color_mode='rgb',\n",
    "        batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 150, 150, 32)      896       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 148, 148, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 74, 74, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 72, 72, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 82944)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               10616960  \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 10,682,915\n",
      "Trainable params: 10,682,915\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "75/75 [==============================] - 495s 7s/step - loss: 1.3399 - acc: 0.3758 - val_loss: 1.0836 - val_acc: 0.3976\n",
      "Epoch 2/25\n",
      "75/75 [==============================] - 513s 7s/step - loss: 1.0824 - acc: 0.4592 - val_loss: 1.0133 - val_acc: 0.5475\n",
      "Epoch 3/25\n",
      "75/75 [==============================] - 520s 7s/step - loss: 1.0367 - acc: 0.4725 - val_loss: 0.9720 - val_acc: 0.5616\n",
      "Epoch 4/25\n",
      "75/75 [==============================] - 527s 7s/step - loss: 1.0221 - acc: 0.4975 - val_loss: 0.9737 - val_acc: 0.5511\n",
      "Epoch 5/25\n",
      "75/75 [==============================] - 506s 7s/step - loss: 0.9883 - acc: 0.5217 - val_loss: 1.1391 - val_acc: 0.3979\n",
      "Epoch 6/25\n",
      "75/75 [==============================] - 545s 7s/step - loss: 0.9591 - acc: 0.5387 - val_loss: 0.9029 - val_acc: 0.5651\n",
      "Epoch 7/25\n",
      "75/75 [==============================] - 492s 7s/step - loss: 0.9192 - acc: 0.5629 - val_loss: 0.9082 - val_acc: 0.5792\n",
      "Epoch 8/25\n",
      "75/75 [==============================] - 497s 7s/step - loss: 0.9000 - acc: 0.5713 - val_loss: 0.8980 - val_acc: 0.6039\n",
      "Epoch 9/25\n",
      "75/75 [==============================] - 482s 6s/step - loss: 0.8796 - acc: 0.5746 - val_loss: 0.8358 - val_acc: 0.6109\n",
      "Epoch 10/25\n",
      "75/75 [==============================] - 480s 6s/step - loss: 0.8691 - acc: 0.6058 - val_loss: 0.8324 - val_acc: 0.6215\n",
      "Epoch 11/25\n",
      "75/75 [==============================] - 480s 6s/step - loss: 0.8126 - acc: 0.6208 - val_loss: 0.8075 - val_acc: 0.6285\n",
      "Epoch 12/25\n",
      "75/75 [==============================] - 478s 6s/step - loss: 0.8220 - acc: 0.6362 - val_loss: 0.8021 - val_acc: 0.6496\n",
      "Epoch 13/25\n",
      "75/75 [==============================] - 483s 6s/step - loss: 0.7851 - acc: 0.6421 - val_loss: 0.7630 - val_acc: 0.6690\n",
      "Epoch 14/25\n",
      "75/75 [==============================] - 478s 6s/step - loss: 0.7690 - acc: 0.6579 - val_loss: 0.8888 - val_acc: 0.5898\n",
      "Epoch 15/25\n",
      "75/75 [==============================] - 480s 6s/step - loss: 0.7808 - acc: 0.6417 - val_loss: 0.7333 - val_acc: 0.7007\n",
      "Epoch 16/25\n",
      "75/75 [==============================] - 481s 6s/step - loss: 0.7466 - acc: 0.6758 - val_loss: 0.7440 - val_acc: 0.6567\n",
      "Epoch 17/25\n",
      "75/75 [==============================] - 479s 6s/step - loss: 0.7501 - acc: 0.6575 - val_loss: 0.7847 - val_acc: 0.6356\n",
      "Epoch 18/25\n",
      "75/75 [==============================] - 478s 6s/step - loss: 0.7238 - acc: 0.6813 - val_loss: 0.6478 - val_acc: 0.7183\n",
      "Epoch 19/25\n",
      "75/75 [==============================] - 482s 6s/step - loss: 0.7259 - acc: 0.6879 - val_loss: 0.7763 - val_acc: 0.6338\n",
      "Epoch 20/25\n",
      "75/75 [==============================] - 480s 6s/step - loss: 0.7022 - acc: 0.6904 - val_loss: 0.7514 - val_acc: 0.6632\n",
      "Epoch 21/25\n",
      "75/75 [==============================] - 477s 6s/step - loss: 0.7100 - acc: 0.6846 - val_loss: 0.6717 - val_acc: 0.7130\n",
      "Epoch 22/25\n",
      "75/75 [==============================] - 501s 7s/step - loss: 0.6833 - acc: 0.6946 - val_loss: 0.6414 - val_acc: 0.7025\n",
      "Epoch 23/25\n",
      "75/75 [==============================] - 483s 6s/step - loss: 0.6536 - acc: 0.7125 - val_loss: 0.6319 - val_acc: 0.6954\n",
      "Epoch 24/25\n",
      "75/75 [==============================] - 478s 6s/step - loss: 0.6819 - acc: 0.7058 - val_loss: 0.6423 - val_acc: 0.7289\n",
      "Epoch 25/25\n",
      "75/75 [==============================] - 478s 6s/step - loss: 0.6634 - acc: 0.7071 - val_loss: 0.6052 - val_acc: 0.7183\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x207817f7518>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',input_shape=train_generator.image_shape,activation='relu'))\n",
    "model.add(Conv2D(32, (3, 3),activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, (3, 3), padding='same',activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3),activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dropout(0.5)) \n",
    "model.add(Dense(len(train_generator.class_indices),activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy']) # Let's train the model using RMSprop\n",
    "model.summary()\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator.classes)//train_generator.batch_size, #samples//batch_size\n",
    "        epochs=25,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator.classes)//validation_generator.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix \n",
    "Y_pred_1 = model.predict_generator(validation_generator,len(validation_generator.classes)//validation_generator.batch_size+1)\n",
    "y_pred_1 = np.argmax(Y_pred_1, axis=1)\n",
    "confusion_matrix = confusion_matrix(validation_generator.classes, y_pred_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[56, 67, 77],\n",
       "       [68, 65, 67],\n",
       "       [48, 79, 73]], dtype=int64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparando ambas matrices de confusión, notamos que la segunda tiene un rendimiento peor, sin embargo, no difiere tanto de la primera que era la que tenia los datos bien etiquetados.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy validation:  0.7083333333333334\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy validation: \",model.evaluate_generator(generator=validation_generator, steps=4)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy validation:  0.7708333333333334\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy validation: \",model.evaluate_generator(generator=validation_generator, steps=3)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy validation:  0.796875\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy validation: \",model.evaluate_generator(generator=validation_generator, steps=2)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy validation:  0.6875\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy validation: \",model.evaluate_generator(generator=validation_generator, steps=1)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tal y como se esperaba el accuracy disminuye en relacion a la CNN que utiliza los datos bien clasificados.  Sin embargo, esta disminución no es tan grande debido a que las imagenes que se intercambiaron (hamburguesas y hot-dogs) pueden ser muy parecidas.  \n",
    "Por otra parte, pese a que el accuracy bajó, aún así sigue siendo mucho mejor hacer una CNN con datos mal etiquetados que una red neuronal Feed Forward con todos los datos bien clasificados.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"refs\"></a>\n",
    "## Referencias\n",
    "[1] http://archive.ics.uci.edu/ml/datasets/Wine+Quality  \n",
    "[2] http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html  \n",
    "[3] http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html, http://colah.github.io/posts/2014-07-Understanding-Convolutions/    \n",
    "[4] https://www.vision.ee.ethz.ch/datasets_extra/food-101/  \n",
    "[5] Box, G. E. P., Jenkins, G. M. and Reinsel, G. C. (1976), *Time Series Analysis, Forecasting and Control*. Third Edition. Holden-Day. Series G.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
